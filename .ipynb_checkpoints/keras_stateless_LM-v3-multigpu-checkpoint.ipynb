{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "cardtext = [list(x) for x in list(np.load('data/card_texts_new.npy'))]\n",
    "c2i = np.load('data/c2i.npy').item()\n",
    "i2c = np.load('data/i2c.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - randomize!\n",
    "np.random.seed = 1337\n",
    "indices = list(np.random.permutation(len(cardtext)))\n",
    "cardtext = [cardtext[i] for i in indices]\n",
    "# cardtext = cardtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "DROP_RATE = 0.2 # dropout\n",
    "EMBEDDING_SIZE = 128 # embedding size\n",
    "HIDDEN_SIZE = 192 # lstm feature vector\n",
    "HIDDEN_LAYERS = 3 # number of layers\n",
    "START_EPOCH = 0\n",
    "VOCAB_SIZE = len(c2i.keys()) # number of characters\n",
    "\n",
    "WINDOW_SIZE = 100 # context length\n",
    "BATCH_SIZE = 64\n",
    "CARDS_PER_BATCH = 5\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "OUT_INCREMENT = 100 # printout after n batches - and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardGenerator(cardtext, windowsize, cards_per_batch, c2i=c2i, debug=False):\n",
    "    \n",
    "    i = 0\n",
    "    indices = list(np.random.permutation(len(cardtext)))\n",
    "    idx = indices[i]\n",
    "    lastseq = ''\n",
    "    \n",
    "    def nextcard(cardtext, idx, debug=debug):\n",
    "        if debug:\n",
    "            card_idx = cardtext[idx]\n",
    "        else:\n",
    "            card_idx = [c2i[c] for c in cardtext[idx]]\n",
    "        \n",
    "        return list(card_idx)\n",
    "    \n",
    "    # pregenerate warmup sequence of windowsize\n",
    "    # get startup sequence with card longer than windowsize+1\n",
    "    while len(cardtext[idx]) < windowsize:\n",
    "        i += 1\n",
    "        idx = indices[i]\n",
    "    # 'pad' sequence with end of card\n",
    "    if debug:\n",
    "        sequence = list(cardtext[idx][-(windowsize):])\n",
    "    else:\n",
    "        sequence = list([c2i[c] for c in cardtext[idx][-(windowsize):]])\n",
    "    i += 1\n",
    "    idx = indices[i]\n",
    "    # add n cards to sequence where n = cards_per_batch\n",
    "    for j in range(cards_per_batch):\n",
    "        sequence += nextcard(cardtext, idx)\n",
    "        i += 1\n",
    "        idx = indices[i]\n",
    "        \n",
    "    # create matrix\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    # main iterator\n",
    "    while True:\n",
    "        \n",
    "        # set lastseq for next cycle\n",
    "        laststr = sequence[-(windowsize):]\n",
    "\n",
    "        # generate batch (of cards_per_batch cards)\n",
    "        while len(sequence) > windowsize:\n",
    "            x.append(np.array(sequence[:windowsize]))\n",
    "            y.append(sequence[windowsize])\n",
    "            sequence.pop(0)\n",
    "        \n",
    "        # generate window-shifted data\n",
    "        # reshape for sparse_categorical_crossentropy\n",
    "        sequence = []\n",
    "        y = np.array(y)\n",
    "        y = y[:, np.newaxis]\n",
    "        # yield and reset\n",
    "        yield(np.asarray(x), y)\n",
    "        x, y = [], []\n",
    "        \n",
    "        # for next batch, check if enough remaining, else reset\n",
    "        if len(indices[i:]) < cards_per_batch:\n",
    "            indices = np.random.permutation(len(cardtext))\n",
    "            i = 0\n",
    "            idx = indices[i]\n",
    "        else:\n",
    "            i += 1\n",
    "            idx = indices[i]\n",
    "            \n",
    "        # seed warmup sequence with end of last batch\n",
    "        sequence = list(laststr)\n",
    "\n",
    "        i += 1\n",
    "        idx = indices[i]\n",
    "        for j in range(cards_per_batch):\n",
    "            sequence += nextcard(cardtext, idx)\n",
    "            i += 1\n",
    "            idx = indices[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "getbatch = cardGenerator(cardtext, WINDOW_SIZE, CARDS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model on CPU\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE, \n",
    "                        input_shape=(WINDOW_SIZE, )))\n",
    "    model.add(Dropout(DROP_RATE))\n",
    "    for _ in range(HIDDEN_LAYERS-1):\n",
    "        model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "    model.add(LSTM(HIDDEN_SIZE))\n",
    "    model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "# make multi-gpu model with 2 GPU's\n",
    "model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 'Ⓔ'\n",
    "\n",
    "def predict(startchars='random', temperature=0.5, maxlen=800):\n",
    "    \n",
    "    seq_out = []\n",
    "    \n",
    "    if temperature=='random':\n",
    "        tmp = np.random.random()\n",
    "    else:\n",
    "        tmp = temperature\n",
    "    \n",
    "    # starting sequence\n",
    "    if startchars=='none':\n",
    "        seq_in = [c2i['Ⓔ'] for i in range(WINDOW_SIZE)]\n",
    "    \n",
    "    elif startchars=='random':\n",
    "        seq_in = []\n",
    "        alpha = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "                 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
    "                 'w', 'x', 'y', 'z']\n",
    "        alpha = [a for a in alpha if a in c2i.keys()]\n",
    "        while len(seq_in) < WINDOW_SIZE-1:\n",
    "            rnd = np.random.randint(0, len(alpha))\n",
    "            seq_in += [c2i[alpha[rnd]]]\n",
    "        seq_in += [c2i['Ⓔ']]\n",
    "    \n",
    "    else:\n",
    "        s = list(startchars)\n",
    "        s = s[:WINDOW_SIZE]\n",
    "        seq_out =  [c2i[c] for c in s]\n",
    "        while len(s) < WINDOW_SIZE:\n",
    "            s.insert(0, 'Ⓔ')\n",
    "        seq_in = [c2i[c] for c in s]\n",
    "        \n",
    "    # softmax temperature\n",
    "    # scaling factor of logits = logits/temperature\n",
    "    # high temp = more confident = more diverse, more mistakes\n",
    "    # low temp: more conservative\n",
    "    # https://stackoverflow.com/questions/37246030/how-to-change-the-temperature-of-a-softmax-output-in-keras/37254117#37254117\n",
    "    def sample(a, temperature=tmp):\n",
    "        a = np.array(a)**(1/temperature)\n",
    "        p_sum = a.sum()\n",
    "        sample_temp = a/p_sum \n",
    "\n",
    "        # stupid fix for > 1 error\n",
    "        while sum(sample_temp) > 1:\n",
    "            sample_temp[0] -= 0.0001\n",
    "\n",
    "        return np.argmax(np.random.multinomial(1, sample_temp, 1))\n",
    "\n",
    "    for i in range(maxlen):\n",
    "\n",
    "        # predict next char\n",
    "        pred_out = model.predict(np.array(seq_in).reshape((1, WINDOW_SIZE)))\n",
    "        # get index of highest pred\n",
    "        idx = sample(pred_out[0])\n",
    "        # save index for decoding\n",
    "        seq_out.append(idx)\n",
    "        # add index to input sequence\n",
    "        seq_in.append(int(idx))\n",
    "        # remove earliest\n",
    "        seq_in.pop(0)\n",
    "\n",
    "    # decode final sequence\n",
    "    card_char = ''.join([i2c[int(i)] for i in seq_out])\n",
    "    card_char = card_char.replace('·', '|')\n",
    "    card_char = card_char.replace('Ⓔ', 'Ⓔ\\n|')\n",
    "    card_text = card_char.split('|')\n",
    "\n",
    "    for f in card_text:\n",
    "        print(f)\n",
    "        \n",
    "    return card_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_weights('model/v4-multi2-modelweights-epoch6-cards5000-2018-02-05 17:27:15.520281.h5')\n",
    "START_EPOCH = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch 1 2018-02-05 19:27:12.710068\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/replica_0/sequential_1/embedding_1/Gather_grad/Shape with an op embedding_1/embeddings/read that had a different device: /device:GPU:0 vs /device:CPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/replica_0/sequential_1/embedding_1/Gather_grad/ToInt32 with an op embedding_1/embeddings/read that had a different device: /device:GPU:0 vs /device:CPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/replica_1/sequential_1/embedding_1/Gather_grad/Shape with an op embedding_1/embeddings/read that had a different device: /device:GPU:1 vs /device:CPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/replica_1/sequential_1/embedding_1/Gather_grad/ToInt32 with an op embedding_1/embeddings/read that had a different device: /device:GPU:1 vs /device:CPU:0. Ignoring colocation property.\n",
      "epoch 7 batch 2 2018-02-05 19:27:20.174926\n",
      "epoch 7 batch 3 2018-02-05 19:27:26.792809\n",
      "epoch 7 batch 4 2018-02-05 19:27:30.272326\n",
      "epoch 7 batch 5 2018-02-05 19:27:35.306334\n",
      "epoch 7 batch 6 2018-02-05 19:27:39.697476\n",
      "epoch 7 batch 7 2018-02-05 19:27:44.956613\n",
      "epoch 7 batch 8 2018-02-05 19:27:49.305318\n",
      "epoch 7 batch 9 2018-02-05 19:27:52.763251\n",
      "epoch 7 batch 10 2018-02-05 19:27:57.119972\n"
     ]
    }
   ],
   "source": [
    "# just train, fix epochs later\n",
    "for epoch_idx in range(START_EPOCH, NUM_EPOCHS):\n",
    "    # epochs are \"meaningless\" here, roughly batch size = n cards so...\n",
    "    for batch in range(int(len(cardtext)/CARDS_PER_BATCH)):\n",
    "        print('epoch', epoch_idx+1, 'batch',batch+1, str(datetime.now()))\n",
    "        # get batch\n",
    "        x_batch, y_batch = next(getbatch)\n",
    "        \n",
    "        # fit to card batch\n",
    "        r = model.fit(x_batch, y_batch, batch_size=BATCH_SIZE, epochs=1, verbose=0)\n",
    "\n",
    "        if batch % OUT_INCREMENT == 0 and batch > 0:\n",
    "            model.save_weights('model/v4-multi2-modelweights-epoch{}-cards{}-{}.h5'.format(epoch_idx+1, CARDS_PER_BATCH*(batch), str(datetime.now())))\n",
    "            # change to writing to file as well as printing\n",
    "            a = predict(startchars='random', temperature='random')\n",
    "            b = predict(startchars='random', temperature='random')\n",
    "            c = predict(startchars='random', temperature='random')\n",
    "            filename = 'sample/v4-multi2-cardsamples-epoch{}-cards{}-{}.txt'.format(epoch_idx+1, CARDS_PER_BATCH*(batch), str(datetime.now()))\n",
    "            print(\"\\nwriting file\", filename)\n",
    "            with open(filename, 'w') as f:\n",
    "                for prd in [a, b, c]:\n",
    "                    for ln in prd:\n",
    "                        f.write(ln)\n",
    "                        f.write('\\n')\n",
    "                f.write('\\n')\n",
    "            \n",
    "            print('***************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # todo: just save to json one time\n",
    "# model.save('model/100test_model.h5')\n",
    "# print(\"saved model to disk\\n\")\n",
    "# model.save_weights('model/100test_model_weights.h5')\n",
    "# print(\"saved model weights to disk\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model.load_weights('model/100-modelweights-epoch71-batch99.h5')\n",
    "# START_EPOCH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(startchars='random', temperature='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
