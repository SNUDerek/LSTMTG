{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras training\n",
    "\n",
    "this file loads our processed data and trains a recurrent language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here, if using multi-gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "cardtext = [list(x) for x in list(np.load('data/card_texts.npy'))]\n",
    "c2i = np.load('data/c2i.npy').item()\n",
    "i2c = np.load('data/i2c.npy').item()\n",
    "xcards = np.load('data/xcards.npy')\n",
    "ycards = np.load('data/ycards.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add axis for sparse_categorical_Crossentropy\n",
    "ycards = ycards[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the hyperparameters\n",
    "\n",
    "**dropout rate** : prevents over-fitting; by 'ignoring' every few characters, the language model must learn to generalize. typical values are 0.25 to 0.50  \n",
    "**embedding size** : the size of the character embeddings, which are learned through training  \n",
    "**hidden_size** : the size of the LSTM gates and cells; i.e. the size of its 'memory'  \n",
    "**vocab_size** : the model will predict one of *n* characters where *n* is the vocabulary size  \n",
    "**batch size** : we will use *minibatch gradient descent*; this is the number of examples we will train on each batch  \n",
    "**number of epochs** : one *epoch* is one pass through all the training data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "DROP_RATE = 0.50              # dropout: between 0.25 and 0.5 is common\n",
    "EMBEDDING_SIZE = 500          # character embedding size\n",
    "HIDDEN_SIZE = 1000            # lstm feature vector size\n",
    "MAX_Y_LEN = ycards.shape[1]   # maximum card length\n",
    "VOCAB_SIZE = len(c2i.keys())  # number of characters\n",
    "BATCH_SIZE = 32               # cards per batch\n",
    "NUM_EPOCHS = 10               # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the model\n",
    "\n",
    "the model we will use is a *recurrent language model*. essentially, our network will predict the next character, given the previous chracters it has seen/generated. we could try to help the network realize that it is at the beginning of a card by initializing the states to a fixed value such as zeroes (using `initial_state`), but we will leave the state initialization random here, and rely on the initial start-of-sentence token to signal to the network that we are starting a card. hopefully, the random initial state might help randomize the generated cards.\n",
    "\n",
    "we have already divided the cards into lists of characters, *indexed* the strings into integer arrays, and *padded* the arrays to a fixed length in the previous files. we also created input and output sequences that are offset by one, such that the first element of the *output* corresponds with the *second* element of the input etc. this is because we will train the model with *teacher forcing* : at each step, we will input the *true* character, and induce the network to output the next element. on decode, of course, since we are randomly generating the card sequences, we must input the *actual* previous output.\n",
    "\n",
    "due to this, our training and decode networks are slightly different. our training network takes full sequence inputs, and outputs full sequence outputs (the inputs, offset by one). this is because we already know the full sequences we are training on: the actual cards. on decode, we want new cards, so we will generate each character at a time, and feed *that* predicted character (sampled randomly from the softmax distribution, for randomness) back into the LSTM to generate the next character. because the LSTM relies on a 'memory' of what it has already generated, we must also input the previous *states*, which we can do with `return_sequences=True`.\n",
    "\n",
    "this network is adapted from the decoder in [the keras blog seeq2seq article](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_input  = Input(shape=(MAX_Y_LEN, ), name='lm_input')\n",
    "decoder_embed  = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, \n",
    "                           mask_zero=True, trainable=True, name='lm_emb')\n",
    "decoder_lstm1  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm1')\n",
    "decoder_lstm2  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm2')\n",
    "\n",
    "decoder_dense_1  = Dense(HIDDEN_SIZE, activation='relu', name='lm_dns_1')\n",
    "decoder_dense_2  = Dense(VOCAB_SIZE, activation='softmax', name='lm_dns_final')\n",
    "\n",
    "x = decoder_embed(decoder_input)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h1, c1 = decoder_lstm1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h2, c2 = decoder_lstm2(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_2(x)\n",
    "\n",
    "model = Model(decoder_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lm_input (InputLayer)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "lm_emb (Embedding)           (None, 256, 500)          51000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256, 500)          0         \n",
      "_________________________________________________________________\n",
      "lm_lstm1 (LSTM)              [(None, 256, 1000), (None 6004000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256, 1000)         0         \n",
      "_________________________________________________________________\n",
      "lm_lstm2 (LSTM)              [(None, 256, 1000), (None 8004000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256, 1000)         0         \n",
      "_________________________________________________________________\n",
      "lm_dns_1 (Dense)             (None, 256, 1000)         1001000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256, 1000)         0         \n",
      "_________________________________________________________________\n",
      "lm_dns_final (Dense)         (None, 256, 102)          102102    \n",
      "=================================================================\n",
      "Total params: 15,162,102\n",
      "Trainable params: 15,162,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training\n",
    "\n",
    "we define a `ModelCheckpoint` that will save models as we train, in case the model training takes a long time. we then `fit` the model to train. we use `verbose=2` to view per-epoch stats; `verbose=1`, while it provides per-batch stats, can freeze Jupyter Lab, and `TQDMNotebook` doesn't work with Jupyter Lab yet (AFAIK).\n",
    "\n",
    "we then save weights at the end of training (and re-load them to test). we have a (commented by default) cell for loading weights before training, to allow us to continue training a partially-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpoint = ModelCheckpoint('model/weights.{epoch:04d}-{loss:.4f}.h5', \n",
    "                         monitor='loss',\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True,\n",
    "                        period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('model/weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 944s - loss: 1.3332 - acc: 0.6142\n",
      "Epoch 2/10\n",
      " - 942s - loss: 0.6561 - acc: 0.7951\n",
      "Epoch 3/10\n",
      " - 942s - loss: 0.5724 - acc: 0.8187\n",
      "Epoch 4/10\n",
      " - 942s - loss: 0.5287 - acc: 0.8317\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(xcards, ycards, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          callbacks=[cpoint], \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model/weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
