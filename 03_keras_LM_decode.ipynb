{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoding\n",
    "\n",
    "use the model to predict some new cards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Activation, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "c2i = np.load('data/c2i.npy').item()\n",
    "i2c = np.load('data/i2c.npy').item()\n",
    "ycards = np.load('data/ycards.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters\n",
    "\n",
    "although dropout, batch size and epochs aren't used at decode, we need the variables that define the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from training\n",
    "DROP_RATE = 0.00\n",
    "\n",
    "HIDDEN_SIZE = 500             # lstm feature vector size\n",
    "MAX_Y_LEN = ycards.shape[1]   # maximum card length\n",
    "VOCAB_SIZE = len(c2i.keys())  # number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_input  = Input(shape=(MAX_Y_LEN, ), name='lm_input')\n",
    "decoder_embed  = Embedding(VOCAB_SIZE, HIDDEN_SIZE, \n",
    "                           mask_zero=True, trainable=True, name='lm_emb')\n",
    "decoder_lstm1  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm1')\n",
    "decoder_lstm2  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm2')\n",
    "\n",
    "decoder_dense_1  = Dense(HIDDEN_SIZE, activation='relu', name='lm_dns_1')\n",
    "\n",
    "def weight_tying(layer_input):\n",
    "    result = K.dot(layer_input, K.transpose(decoder_embed.weights[0]))\n",
    "    return result\n",
    "\n",
    "decoder_dense_2 = Lambda(weight_tying, name='weight_tying')\n",
    "decoder_dense_3 = Activation('softmax')\n",
    "\n",
    "x = decoder_embed(decoder_input)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h1, c1 = decoder_lstm1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h2, c2 = decoder_lstm2(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_2(x)\n",
    "x = decoder_dense_3(x)\n",
    "\n",
    "model = Model(decoder_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/weights_tiedfinal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this input is for the previously-predicted character\n",
    "decoder_input  = Input(shape=(1, ))\n",
    "# these inputs are the recurrent states\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_states_inputs1 = [decoder_state_input_h1, decoder_state_input_c1]\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_states_inputs2 = [decoder_state_input_h2, decoder_state_input_c2]\n",
    "\n",
    "# we reuse the embedding layer\n",
    "x = decoder_embed(decoder_input)\n",
    "x, dh1, dc1 = decoder_lstm1(x, initial_state=decoder_states_inputs1)\n",
    "decoded_states1 = [dh1, dc1]\n",
    "x, dh2, dc2 = decoder_lstm2(x, initial_state=decoder_states_inputs2)\n",
    "decoded_states2 = [dh2, dc2]\n",
    "\n",
    "x = decoder_dense_1(x)\n",
    "x = decoder_dense_2(x)\n",
    "x = decoder_dense_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = Model(inputs=[decoder_input] + decoder_states_inputs1 + decoder_states_inputs2, \n",
    "                  outputs=[x] + decoded_states1 + decoded_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_emb (Embedding)              multiple             51000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_lstm1 (LSTM)                 multiple             2002000     lm_emb[1][0]                     \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_lstm2 (LSTM)                 multiple             2002000     lm_lstm1[1][0]                   \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lm_dns_1 (Dense)                multiple             250500      lm_lstm2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "weight_tying (Lambda)           multiple             0           lm_dns_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       multiple             0           weight_tying[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,305,500\n",
      "Trainable params: 4,305,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode function\n",
    "\n",
    "we initialize the states randomly, and start our sequence qith the SOS character. until we reach a set length or we reach an end-of-sequence character, we will generate a probability distribution over the next predicted characters, sample a character randomly according to the distribution (we won't use a greedy or beam-search method because we *want* a degree of 'wackiness' in this case), and input that character (along with the LSTM previous states) *back* into the model to generate another character, etc.\n",
    "\n",
    "the *temperature* scales the softmax distribution, allowing for more or less randomness in the network predictions. a temperature of 1 is unscaled, a temperature above one means that the relative probabilities are closer (and thus the network is more 'random'), while temperatures below 1 make the network more confident (and thus more 'conservative')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(temperature=1.0, maxlen=256, seed=None, debug=False):\n",
    "    # randomize input state vectors.\n",
    "    a = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    b = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    c = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    d = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    states1 = [a, b]\n",
    "    states2 = [c, d]\n",
    "    decoded_sentence = []\n",
    "    # Generate empty target sequence of length 1.\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # add seed if present\n",
    "    target_seq = [c2i['Ⓢ']]\n",
    "    if seed is not None:\n",
    "        seed = seed.lower()\n",
    "        if seed[-1] != '⒞':\n",
    "            seed += '⒞'\n",
    "        for char in seed:\n",
    "            target_seq.append(c2i[char])\n",
    "            \n",
    "    # pre-load seed - run loop without saving output\n",
    "    # and use target output instead; akin to teacher forcing\n",
    "    if len(target_seq) > 1:\n",
    "        for i in range(len(target_seq)-1):\n",
    "            output_tokens, h1, c1, h2, c2 = gen_model.predict([np.array([target_seq[i]])] + states1 + states2)\n",
    "            states1 = [h1, c1]\n",
    "            states2 = [h2, c2]\n",
    "        for c in seed:\n",
    "            decoded_sentence.append(c)\n",
    "                \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        if debug:\n",
    "            print('inp:', [np.array([target_seq[-1]])])\n",
    "            print('st1:', np.shape(states1))\n",
    "            print('st2:', np.shape(states2))\n",
    "            \n",
    "        output_tokens, h1, c1, h2, c2 = gen_model.predict([np.array([target_seq[-1]])] + states1 + states2)\n",
    "        if debug:\n",
    "            # print('typ:', type(output_tokens), type(h1), type(c2), type(h2), type(c2))\n",
    "            print('out:', output_tokens.shape)\n",
    "            print('max:', i2c[target_seq[-1]], '=>', i2c[np.argmax(output_tokens)])\n",
    "            print()\n",
    "        \n",
    "        # Update states\n",
    "        states1 = [h1, c1]\n",
    "        states2 = [h2, c2]\n",
    "        \n",
    "        def sample(a, temperature=temperature):\n",
    "            a = np.array(a)**(1/temperature)\n",
    "            p_sum = a.sum()\n",
    "            sample_temp = a/p_sum \n",
    "            # stupid fix for > 1 error\n",
    "            while sum(sample_temp) > 1:\n",
    "                sample_temp[0] -= 0.0001\n",
    "            return np.argmax(np.random.multinomial(1, sample_temp, 1))\n",
    "        \n",
    "        # Sample a token with temperature\n",
    "        sampled_token_index = idx = sample(np.squeeze(output_tokens))\n",
    "        sampled_char = i2c[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_sentence) > maxlen*2 or sampled_char in ['Ⓔ']:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq.append(sampled_token_index)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate extension idea:\n",
    "\n",
    "you could allow users to 'seed' any number of fields by eliciting the targets. e.g. the user wants to make themselves as a card, so they choose:  \n",
    "`name` = \"billy bob\"  \n",
    "`type` = \"Legendary Creature - Human\"\n",
    "\n",
    "during the generation loop, when the leading tag for that field is generated, e.g. `⒯` for `type`, then just like the current code, the *desired* input is 'forced' into the LSTM until the end of that field (so for type we would end the sequence with `⒫` to signal the *end* of the type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ⓢ for name (at start of card)\n",
    "# ⒞ for mana cost\n",
    "# ⒭ for rarity\n",
    "# ⒯ for type & subtype\n",
    "# ⒫ for power & toughness\n",
    "# ⒜ for each ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(temperature=1, seed=None):\n",
    "    card = ''.join(decode_sequence(temperature=temperature, seed=seed)).replace('Ⓔ', '')\n",
    "    rarity = {'S': 'promo', 'M': 'mythic', 'C': 'common', 'U': 'uncommon', 'R': 'rare'}\n",
    "    splits = ['⒞', '⒭', '⒯', '⒫', '⒜']\n",
    "    for s in splits:\n",
    "        card = card.replace(s, '|'+s)\n",
    "    card = card.split('|')\n",
    "    \n",
    "    cardd = {}\n",
    "    cardd['abil'] = []\n",
    "    for i, l in enumerate(card):\n",
    "        if i == 0:\n",
    "            cardd['name'] = card[i].title()\n",
    "        else:\n",
    "            l = l.replace('Ⓝ', cardd['name'])\n",
    "            if '⒞' in l:\n",
    "                cardd['cost'] = l.replace('⒞', '')\n",
    "            elif '⒭' in l:\n",
    "                cardd['rare'] = rarity.get(l.replace('⒭', ''), 'unknown')\n",
    "            elif '⒯' in l:\n",
    "                cardd['type'] = l.replace('⒯', '').replace(':', ': ').replace('·', ' ')\n",
    "            elif '⒫' in l:\n",
    "                if 'creature' in cardd['type']:\n",
    "                    cardd['pt'] = l.replace('⒫', '')\n",
    "            else:\n",
    "                cardd['abil'].append(l.replace('⒜', '').replace(' x ', ' X '))\n",
    "    \n",
    "    for cat in ['name', 'cost', 'rare', 'type', 'abil', 'pt']:\n",
    "        if cat in cardd.keys():\n",
    "            if cat == 'abil':\n",
    "                for a in cardd[cat]:\n",
    "                    print(a)\n",
    "            else:\n",
    "                print(cardd[cat])\n",
    "    return cardd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples\n",
    "\n",
    "here we generate some cards with different temperature settings\n",
    "\n",
    "i did cheat here to generate a 'well-formed' card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colanatic Spirit\n",
      "③Ⓦ\n",
      "common\n",
      "creature: elf shaman\n",
      "whenever Colanatic Spirit deals combat damage to a player, you may pay ①. if you do, target creature gets +2/+2 until end of turn.\n",
      "1/1\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odremon Imp\n",
      "②Ⓖ\n",
      "uncommon\n",
      "creature: vampire\n",
      "②Ⓖ: Odremon Imp gains protection from the color of your choice until end of turn.\n",
      "2/1\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oatodra-Hey Dranrer\n",
      "④Ⓤ\n",
      "uncommon\n",
      "creature: chamber spirit\n",
      "lifelink\n",
      "4/6\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puy Awl U6Pl, Bjoyrkva\n",
      "⑪ⓊⓊⓊ\n",
      "creature: elfrakaun julcuklcihaPuy Awl U6Pl, Bjoyrkva\n",
      "R\n",
      "RⓇⓊ: buflova Puy Awl U6Pl, Bjoyrkva extun\n",
      "3/3\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with some name seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Rosewater\n",
      "②Ⓦ\n",
      "common\n",
      "creature: dryad\n",
      "\n",
      "2/2\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1, seed='Mark Rosewater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Garfield\n",
      "Ⓖ\n",
      "uncommon\n",
      "creature: djinn\n",
      "flying\n",
      "5/5\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1, seed='Richard Garfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gavin Verhey\n",
      "②Ⓤ\n",
      "common\n",
      "creature: human assassin\n",
      "when Gavin Verhey enters the battlefield, sacrifice Gavin Verhey.\n",
      "2/3\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1, seed='Gavin Verhey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melissa Detora\n",
      "④ⒷⒷ\n",
      "promo\n",
      "creature: screat soldier\n",
      "\n",
      "2/2\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1, seed='Melissa DeTora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaby Spartz\n",
      "③Ⓖ\n",
      "rare\n",
      "creature: cat guard\n",
      "sacrifice Gaby Spartz: draw a card.\n",
      "3/3\n"
     ]
    }
   ],
   "source": [
    "c = generate(temperature=1, seed='Gaby Spartz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
