{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoding\n",
    "\n",
    "use the model to predict some new cards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "c2i = np.load('data/c2i.npy').item()\n",
    "i2c = np.load('data/i2c.npy').item()\n",
    "ycards = np.load('data/ycards.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters\n",
    "\n",
    "although dropout, batch size and epochs aren't needed, we need the variables that define the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from training\n",
    "DROP_RATE = 0.33\n",
    "\n",
    "EMBEDDING_SIZE = 400          # character embedding size\n",
    "HIDDEN_SIZE = 800             # lstm feature vector size\n",
    "MAX_Y_LEN = ycards.shape[1]   # maximum card length\n",
    "VOCAB_SIZE = len(c2i.keys())  # number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_input  = Input(shape=(MAX_Y_LEN, ), name='lm_input')\n",
    "decoder_embed  = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, \n",
    "                           mask_zero=True, trainable=True, name='lm_emb')\n",
    "decoder_lstm1  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm1')\n",
    "decoder_lstm2  = LSTM(HIDDEN_SIZE, \n",
    "                      return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      name='lm_lstm2')\n",
    "\n",
    "decoder_dense_1  = Dense(HIDDEN_SIZE, activation='relu', name='lm_dns_1')\n",
    "decoder_dense_2  = Dense(VOCAB_SIZE, activation='softmax', name='lm_dns_final')\n",
    "\n",
    "x = decoder_embed(decoder_input)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h1, c1 = decoder_lstm1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x, h2, c2 = decoder_lstm2(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_1(x)\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = decoder_dense_2(x)\n",
    "\n",
    "model = Model(decoder_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this input is for the previously-predicted character\n",
    "decoder_input  = Input(shape=(1, ))\n",
    "# these inputs are the recurrent states\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_states_inputs1 = [decoder_state_input_h1, decoder_state_input_c1]\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_states_inputs2 = [decoder_state_input_h2, decoder_state_input_c2]\n",
    "\n",
    "# we reuse the embedding layer\n",
    "x = decoder_embed(decoder_input)\n",
    "x, dh1, dc1 = decoder_lstm1(x, initial_state=decoder_states_inputs1)\n",
    "decoded_states1 = [dh1, dc1]\n",
    "x, dh2, dc2 = decoder_lstm2(x, initial_state=decoder_states_inputs2)\n",
    "decoded_states2 = [dh2, dc2]\n",
    "\n",
    "x = decoder_dense_1(x)\n",
    "x = decoder_dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = Model(inputs=[decoder_input] + decoder_states_inputs1 + decoder_states_inputs2, \n",
    "                  outputs=[x] + decoded_states1 + decoded_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_emb (Embedding)              multiple             39600       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 800)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 800)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_lstm1 (LSTM)                 multiple             3843200     lm_emb[1][0]                     \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 800)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 800)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lm_lstm2 (LSTM)                 multiple             5123200     lm_lstm1[1][0]                   \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lm_dns_1 (Dense)                multiple             640800      lm_lstm2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lm_dns_final (Dense)            multiple             79299       lm_dns_1[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,726,099\n",
      "Trainable params: 9,726,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode function\n",
    "\n",
    "we initialize the states randomly, and start our sequence qith the SOS character. until we reach a set length or we reach an end-of-sequence character, we will generate a probability distribution over the next predicted characters, sample a character randomly according to the distribution (we won't use a greedy or beam-search method because we *want* a degree of 'wackiness' in this case), and input that character (along with the LSTM previous states) *back* into the model to generate another character, etc.\n",
    "\n",
    "the *temperature* scales the softmax distribution, allowing for more or less randomness in the network predictions. a temperature of 1 is unscaled, a temperature above one means that the relative probabilities are closer (and thus the network is more 'random'), while temperatures below 1 make the network more confident (and thus more 'conservative')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(temperature=1.0, maxlen=256, debug=False):\n",
    "    # randomize input state vectors.\n",
    "    a = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    b = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    c = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    d = np.random.random(HIDDEN_SIZE).reshape(1, -1)\n",
    "    states1 = [a, b]\n",
    "    states2 = [c, d]\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = [c2i['Ⓢ']]\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        if debug:\n",
    "            print('inp:', [np.array([target_seq[-1]])])\n",
    "            print('st1:', np.shape(states1))\n",
    "            print('st2:', np.shape(states2))\n",
    "        output_tokens, h1, c1, h2, c2 = gen_model.predict([np.array([target_seq[-1]])] + states1 + states2)\n",
    "        if debug:\n",
    "            # print('typ:', type(output_tokens), type(h1), type(c2), type(h2), type(c2))\n",
    "            print('out:', output_tokens.shape)\n",
    "            print('max:', i2c[target_seq[-1]], '=>', i2c[np.argmax(output_tokens)])\n",
    "            print()\n",
    "        \n",
    "        # Update states\n",
    "        states1 = [h1, c1]\n",
    "        states2 = [h2, c2]\n",
    "        \n",
    "        def sample(a, temperature=temperature):\n",
    "            a = np.array(a)**(1/temperature)\n",
    "            p_sum = a.sum()\n",
    "            sample_temp = a/p_sum \n",
    "            # stupid fix for > 1 error\n",
    "            while sum(sample_temp) > 1:\n",
    "                sample_temp[0] -= 0.0001\n",
    "            return np.argmax(np.random.multinomial(1, sample_temp, 1))\n",
    "        \n",
    "        # Sample a token with temperature\n",
    "        sampled_token_index = idx = sample(np.squeeze(output_tokens))\n",
    "        sampled_char = i2c[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_sentence) > maxlen*2 or sampled_char in ['Ⓔ']:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq.append(sampled_token_index)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(temperature=1):\n",
    "    card = ''.join(decode_sequence(temperature=temperature)).replace('Ⓔ', '').replace('·', '|').split('|')\n",
    "    for l in card:\n",
    "        print(l.replace('Ⓝ', card[0]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples\n",
    "\n",
    "here we generate some cards with different temperature settings\n",
    "\n",
    "i did cheat here to generate a 'well-formed' card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illanter's spike\n",
      "②Ⓖ\n",
      "R\n",
      "sorcery\n",
      "create a 1/1 white knight creature token with haste. exile it at the beginning of the next end step.\n"
     ]
    }
   ],
   "source": [
    "generate(temperature=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oning-archive\n",
      "③Ⓦ\n",
      "U\n",
      "creature\n",
      "angel\n",
      "3\n",
      "3\n",
      "flying\n",
      "②: target creature loses all abilities until your next upkeep.\n"
     ]
    }
   ],
   "source": [
    "generate(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmingpear's rapler\n",
      "⑦\n",
      "C\n",
      "artifact\n",
      "creature\n",
      "hydra\n",
      "4\n",
      "2\n",
      "ⒼⒼ: target creature loses double strike until end of turn.\n"
     ]
    }
   ],
   "source": [
    "generate(temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wumppaiiter\n",
      "①ⓌⓌ\n",
      "M\n",
      "creature\n",
      "dwarfⒷx\n",
      "4\n",
      "2\n",
      "quit,btins ↷, phoucu\n",
      "x't.\n",
      "rix\n",
      "③ⒷⓇg: /19 Ⓤ5.thoslc\n",
      "wumppaiiters . ③\n",
      "Ⓦw5yismwayp\"ucs—②\n",
      "④, sacrifice ↷w eyf.,ctrezibmr—dr\n",
      " unlity i\n"
     ]
    }
   ],
   "source": [
    "generate(temperature=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
